{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6728f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "\n",
    "# Dataset https://www.kaggle.com/code/aadyasingh55/model-training-of-tweet-emotion-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc3e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  i feel awful about it too because it s my job ...      0\n",
      "1                              im alone i feel awful      0\n",
      "2  ive probably mentioned this before but i reall...      1\n",
      "3           i was feeling a little low few days back      0\n",
      "4  i beleive that i am much more sensitive to oth...      2\n",
      "0         0\n",
      "1         0\n",
      "2         1\n",
      "3         0\n",
      "4         2\n",
      "         ..\n",
      "416804    1\n",
      "416805    4\n",
      "416806    0\n",
      "416807    1\n",
      "416808    0\n",
      "Name: label, Length: 416809, dtype: int64\n",
      "                                                text  emotion\n",
      "0  i feel awful about it too because it s my job ...  sadness\n",
      "1                              im alone i feel awful  sadness\n",
      "2  ive probably mentioned this before but i reall...      joy\n",
      "3           i was feeling a little low few days back  sadness\n",
      "4  i beleive that i am much more sensitive to oth...     love\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data.parquet')\n",
    "\n",
    "print(df.head())\n",
    "print(df['label'])\n",
    "\n",
    "# Map the labels to emotion names for better readability (Optional)\n",
    "emotion_map = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
    "df['emotion'] = df['label'].map(emotion_map)\n",
    "\n",
    "# Preview the updated dataframe\n",
    "print(df[['text', 'emotion']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc628890",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants\n",
    "EIGHT_THOUSAND = 8000\n",
    "SIXTEEN_THOUSAND = 16000\n",
    "TRAIN_TEXT = 'train_text.txt'\n",
    "LABEL_FILE = 'train_labels.txt'\n",
    "SAMPLE_TEXT = \"This is a sample sentence used for BPE bits. It can be up to 128 characters long.\"\n",
    "TOKEN_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773bc9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416809, 3)\n",
      "Index(['text', 'label', 'emotion'], dtype='object')\n",
      "Data Types: text       object\n",
      "label       int64\n",
      "emotion    object\n",
      "dtype: object\n",
      "Empty values text       0\n",
      "label      0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "Duplicates: 686\n",
      "label\n",
      "1    141067\n",
      "0    121187\n",
      "3     57317\n",
      "4     47712\n",
      "2     34554\n",
      "5     14972\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    0.338\n",
      "0    0.291\n",
      "3    0.138\n",
      "4    0.114\n",
      "2    0.083\n",
      "5    0.036\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#BASIC PRE PROCESSING\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(f\"Data Types: {df.dtypes}\")\n",
    "print(f\"Empty values {df.isna().sum()}\")\n",
    "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df[\"label\"].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba4e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Distribution (After Stratification)\n",
      "Train(%): label\n",
      "0    29.075\n",
      "1    33.844\n",
      "2     8.290\n",
      "3    13.752\n",
      "4    11.447\n",
      "5     3.592\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Val(%): label\n",
      "0    29.076\n",
      "1    33.845\n",
      "2     8.289\n",
      "3    13.752\n",
      "4    11.446\n",
      "5     3.592\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test(%): label\n",
      "0    29.076\n",
      "1    33.845\n",
      "2     8.292\n",
      "3    13.750\n",
      "4    11.446\n",
      "5     3.592\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"text\"].astype(str)\n",
    "y = df[\"label\"].astype(int)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Second split to obtain validation as well as test set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Feature Distribution (After Stratification)\")\n",
    "print(f\"Train(%): {y_train.value_counts(normalize=True).round(5).sort_index() * 100}\\n\")\n",
    "print(f\"Val(%): {y_val.value_counts(normalize=True).round(5).sort_index() * 100}\\n\")\n",
    "print(f\"Test(%): {y_test.value_counts(normalize=True).round(5).sort_index() * 100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67619352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    333447.000000\n",
      "mean         19.200329\n",
      "std          11.046795\n",
      "min           1.000000\n",
      "25%          11.000000\n",
      "50%          17.000000\n",
      "75%          25.000000\n",
      "max         178.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "text_lengths = X_train.str.split().apply(len)\n",
    "print(text_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5226959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 333447 to file\n"
     ]
    }
   ],
   "source": [
    "X_train_clean = X_train.str.strip()\n",
    "X_val_clean = X_val.str.strip()\n",
    "X_test_clean = X_test.str.strip()\n",
    "\n",
    "X_train_clean.to_csv('train_text.txt', index=False, header=False)\n",
    "print(f\"Wrote {len(X_train_clean)} to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc2bee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8000\n",
      "BPE Pieces: ['▁', 'T', 'h', 'is', '▁is', '▁a', '▁sam', 'ple', '▁sentence', '▁used', '▁for', '▁', 'BPE', '▁bits', '.', '▁', 'I', 't', '▁can', '▁be']\n"
     ]
    }
   ],
   "source": [
    "### Byte Pair Encoding (BPE)\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=TRAIN_TEXT,\n",
    "    model_prefix='m_bpe',\n",
    "    vocab_size=EIGHT_THOUSAND,\n",
    "    model_type='bpe'\n",
    ")\n",
    "\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('m_bpe.model')\n",
    "\n",
    "print(f\"Vocab size: {sp_bpe.get_piece_size()}\")\n",
    "print(f\"BPE Pieces: {sp_bpe.encode(SAMPLE_TEXT, out_type=str)[:20]}\")\n",
    "\n",
    "def encode_texts(sp, texts):\n",
    "  \"\"\"\n",
    "  Encode text, create token IDs and attention masks\n",
    "  \"\"\"\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "\n",
    "  for text in texts:\n",
    "    ids = sp.encode(text, out_type=int)\n",
    "\n",
    "    # reduce tokens to 128 (we have some at 178, but minimal)\n",
    "    ids = ids[:TOKEN_LENGTH]\n",
    "    attention_mask = [1] * len(ids)\n",
    "    \n",
    "    # pad to max_len\n",
    "    pad_id = 3\n",
    "    while len(ids) < TOKEN_LENGTH:\n",
    "      ids.append(pad_id)\n",
    "      ## add padding to the attention mask to ensure each token is 128 bits\n",
    "      attention_mask.append(0)\n",
    "\n",
    "    input_ids.append(ids)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "  return np.array(input_ids, dtype=np.int32), np.array(attention_masks, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a0ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE train shape: ((333447, 128), (333447, 128))\n",
      "BPE val shape: ((41681, 128), (41681, 128))\n",
      "BPE test shape: ((41681, 128), (41681, 128))\n"
     ]
    }
   ],
   "source": [
    "### BPE Generate data\n",
    "\n",
    "Xb_train, att_b_mask_train = encode_texts(sp_bpe, X_train_clean.tolist())\n",
    "Xb_val, att_b_mask_val = encode_texts(sp_bpe, X_val_clean.tolist())\n",
    "Xb_test, att_b_mask_test = encode_texts(sp_bpe, X_test_clean.tolist())\n",
    "\n",
    "print(f\"BPE train shape: {Xb_train.shape, att_b_mask_train.shape}\")\n",
    "print(f\"BPE val shape: {Xb_val.shape, att_b_mask_val.shape}\")\n",
    "print(f\"BPE test shape: {Xb_test.shape, att_b_mask_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BPE TensorFlow input \n",
    "batch_size = 64\n",
    "\n",
    "def make_dataset(X_ids, X_mask, y):\n",
    "  data = tf.data.Dataset.from_tensor_slices(((X_ids, X_mask), y))\n",
    "  data = data.shuffle(10000, reshuffle_each_iteration=True)\n",
    "  data = data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "  return data\n",
    "\n",
    "bpe_train_data = make_dataset(Xb_train, att_b_mask_train, y_train.values)\n",
    "\n",
    "# tf.data.AUTOTUNE allows TensorFlow to automatically determine the optimal number of parallel calls for data loading and preprocessing\n",
    "bpe_val_data = tf.data.Dataset.from_tensor_slices(((Xb_val, att_b_mask_val), y_val.values)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "bpe_test_data = tf.data.Dataset.from_tensor_slices(((Xb_test, att_b_mask_test), y_test.values)).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d9c1a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.5732343809631868, 1: 0.4924503557725537, 2: 2.010436638570343, 3: 1.2119880490251669, 4: 1.455973277443018, 5: 4.639714476540324}\n"
     ]
    }
   ],
   "source": [
    "### BPE - Balance classes\n",
    "classes = np.sort(y_train.unique())\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "\n",
    "class_weights = {int(k): float(v) for k, v in zip(classes, weights)}\n",
    "print(f\"Class weights: {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ce039",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BPE Model Definition\n",
    "def build_transformer_classifier(vocab_size, num_classes, max_len=128, num_heads=4, ff_dim=256, dropout=0.1):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    X = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim=128(input_ids))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
