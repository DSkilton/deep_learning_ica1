{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac6728f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tokenization_and_embedding import TokenAndPositionEmbedding\n",
    "from transformer_block import TransformerBlock\n",
    "\n",
    "# Dataset https://www.kaggle.com/code/aadyasingh55/model-training-of-tweet-emotion-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc3e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  i feel awful about it too because it s my job ...      0\n",
      "1                              im alone i feel awful      0\n",
      "2  ive probably mentioned this before but i reall...      1\n",
      "3           i was feeling a little low few days back      0\n",
      "4  i beleive that i am much more sensitive to oth...      2\n",
      "0         0\n",
      "1         0\n",
      "2         1\n",
      "3         0\n",
      "4         2\n",
      "         ..\n",
      "416804    1\n",
      "416805    4\n",
      "416806    0\n",
      "416807    1\n",
      "416808    0\n",
      "Name: label, Length: 416809, dtype: int64\n",
      "                                                text  emotion\n",
      "0  i feel awful about it too because it s my job ...  sadness\n",
      "1                              im alone i feel awful  sadness\n",
      "2  ive probably mentioned this before but i reall...      joy\n",
      "3           i was feeling a little low few days back  sadness\n",
      "4  i beleive that i am much more sensitive to oth...     love\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data.parquet')\n",
    "\n",
    "print(df.head())\n",
    "print(df['label'])\n",
    "\n",
    "# Map the labels to emotion names for better readability (Optional)\n",
    "emotion_map = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
    "df['emotion'] = df['label'].map(emotion_map)\n",
    "\n",
    "# Preview the updated dataframe\n",
    "print(df[['text', 'emotion']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc628890",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants\n",
    "EIGHT_THOUSAND = 8000\n",
    "SIXTEEN_THOUSAND = 16000\n",
    "TRAIN_TEXT = 'train_text.txt'\n",
    "LABEL_FILE = 'train_labels.txt'\n",
    "SAMPLE_TEXT = \"This is a sample sentence used for BPE bits. It can be up to 128 characters long.\"\n",
    "TOKEN_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "773bc9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416809, 3)\n",
      "Index(['text', 'label', 'emotion'], dtype='object')\n",
      "Data Types: text       object\n",
      "label       int64\n",
      "emotion    object\n",
      "dtype: object\n",
      "Empty values text       0\n",
      "label      0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "Duplicates: 686\n",
      "label\n",
      "1    141067\n",
      "0    121187\n",
      "3     57317\n",
      "4     47712\n",
      "2     34554\n",
      "5     14972\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    0.338\n",
      "0    0.291\n",
      "3    0.138\n",
      "4    0.114\n",
      "2    0.083\n",
      "5    0.036\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#BASIC PRE PROCESSING\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(f\"Data Types: {df.dtypes}\")\n",
    "print(f\"Empty values {df.isna().sum()}\")\n",
    "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df[\"label\"].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ba4e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Distribution (After Stratification)\n",
      "Train(%): label\n",
      "0    29.075\n",
      "1    33.844\n",
      "2     8.290\n",
      "3    13.752\n",
      "4    11.447\n",
      "5     3.592\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Val(%): label\n",
      "0    29.076\n",
      "1    33.845\n",
      "2     8.289\n",
      "3    13.752\n",
      "4    11.446\n",
      "5     3.592\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test(%): label\n",
      "0    29.076\n",
      "1    33.845\n",
      "2     8.292\n",
      "3    13.750\n",
      "4    11.446\n",
      "5     3.592\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"text\"].astype(str)\n",
    "y = df[\"label\"].astype(int)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Second split to obtain validation as well as test set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Feature Distribution (After Stratification)\")\n",
    "print(f\"Train(%): {y_train.value_counts(normalize=True).round(5).sort_index() * 100}\\n\")\n",
    "print(f\"Val(%): {y_val.value_counts(normalize=True).round(5).sort_index() * 100}\\n\")\n",
    "print(f\"Test(%): {y_test.value_counts(normalize=True).round(5).sort_index() * 100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67619352",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_lengths\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "File \u001b[1;32me:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:137\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use .str.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with values of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:919\u001b[0m, in \u001b[0;36mStringMethods.split\u001b[1;34m(self, pat, n, expand, regex)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_re(pat):\n\u001b[0;32m    918\u001b[0m     regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 919\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    921\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mcategories\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32me:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:359\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_split\u001b[1;34m(self, pat, n, expand, regex)\u001b[0m\n\u001b[0;32m    357\u001b[0m             n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    358\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(pat, n)\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:78\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_map\u001b[1;34m(self, f, na_value, dtype, convert)\u001b[0m\n\u001b[0;32m     76\u001b[0m map_convert \u001b[38;5;241m=\u001b[39m convert \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(mask)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_convert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# Reraise the exception if callable `f` got wrong number of args.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# The user may want to be warned by this, instead of getting NaN\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     p_err \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m((takes)|(missing)) (?(2)from \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ to )?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?(3)required )positional arguments?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2895\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mlib.pyx:2932\u001b[0m, in \u001b[0;36mpandas._libs.lib._map_infer_mask\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:337\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_split.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    336\u001b[0m         n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 337\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m     new_pat: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m re\u001b[38;5;241m.\u001b[39mPattern\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_lengths = X_train.str.split().apply(len)\n",
    "print(text_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5226959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 333447 to file\n"
     ]
    }
   ],
   "source": [
    "X_train_clean = X_train.str.strip()\n",
    "X_val_clean = X_val.str.strip()\n",
    "X_test_clean = X_test.str.strip()\n",
    "\n",
    "X_train_clean.to_csv('train_text.txt', index=False, header=False)\n",
    "print(f\"Wrote {len(X_train_clean)} to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2bee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8000\n",
      "BPE Pieces: ['‚ñÅ', 'T', 'h', 'is', '‚ñÅis', '‚ñÅa', '‚ñÅsam', 'ple', '‚ñÅsentence', '‚ñÅused', '‚ñÅfor', '‚ñÅ', 'BPE', '‚ñÅbits', '.', '‚ñÅ', 'I', 't', '‚ñÅcan', '‚ñÅbe']\n"
     ]
    }
   ],
   "source": [
    "### Byte Pair Encoding (BPE)\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=TRAIN_TEXT,\n",
    "    model_prefix='m_bpe',\n",
    "    vocab_size=EIGHT_THOUSAND,\n",
    "    model_type='bpe'\n",
    ")\n",
    "\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('m_bpe.model')\n",
    "\n",
    "print(f\"Vocab size: {sp_bpe.get_piece_size()}\")\n",
    "print(f\"BPE Pieces: {sp_bpe.encode(SAMPLE_TEXT, out_type=str)[:20]}\")\n",
    "\n",
    "def encode_texts(sp, texts):\n",
    "  \"\"\"\n",
    "  Encode text, create token IDs and attention masks\n",
    "  \"\"\"\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "\n",
    "  for text in texts:\n",
    "    ids = sp.encode(text, out_type=int)\n",
    "\n",
    "    # reduce tokens to 128 (we have some at 178, but minimal)\n",
    "    ids = ids[:TOKEN_LENGTH]\n",
    "    attention_mask = [1] * len(ids)\n",
    "    \n",
    "    # pad to max_len\n",
    "    pad_id = 3\n",
    "    while len(ids) < TOKEN_LENGTH:\n",
    "      ids.append(pad_id)\n",
    "      ## add padding to the attention mask to ensure each token is 128 bits\n",
    "      attention_mask.append(0)\n",
    "\n",
    "    input_ids.append(ids)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "  return np.array(input_ids, dtype=np.int32), np.array(attention_masks, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE train shape: ((333447, 128), (333447, 128))\n",
      "BPE val shape: ((41681, 128), (41681, 128))\n",
      "BPE test shape: ((41681, 128), (41681, 128))\n"
     ]
    }
   ],
   "source": [
    "### BPE Generate data\n",
    "\n",
    "Xb_train, att_b_mask_train = encode_texts(sp_bpe, X_train_clean.tolist())\n",
    "Xb_val, att_b_mask_val = encode_texts(sp_bpe, X_val_clean.tolist())\n",
    "Xb_test, att_b_mask_test = encode_texts(sp_bpe, X_test_clean.tolist())\n",
    "\n",
    "print(f\"BPE train shape: {Xb_train.shape, att_b_mask_train.shape}\")\n",
    "print(f\"BPE val shape: {Xb_val.shape, att_b_mask_val.shape}\")\n",
    "print(f\"BPE test shape: {Xb_test.shape, att_b_mask_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BPE TensorFlow input \n",
    "batch_size = 64\n",
    "\n",
    "def make_dataset(X_ids, X_mask, y):\n",
    "  data = tf.data.Dataset.from_tensor_slices(((X_ids, X_mask), y))\n",
    "  data = data.shuffle(10000, reshuffle_each_iteration=True)\n",
    "  data = data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "  return data\n",
    "\n",
    "bpe_train_data = make_dataset(Xb_train, att_b_mask_train, y_train.values)\n",
    "\n",
    "# tf.data.AUTOTUNE allows TensorFlow to automatically determine the optimal number of parallel calls for data loading and preprocessing\n",
    "bpe_val_data = tf.data.Dataset.from_tensor_slices(((Xb_val, att_b_mask_val), y_val.values)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "bpe_test_data = tf.data.Dataset.from_tensor_slices(((Xb_test, att_b_mask_test), y_test.values)).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c1a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.5732343809631868, 1: 0.4924503557725537, 2: 2.010436638570343, 3: 1.2119880490251669, 4: 1.455973277443018, 5: 4.639714476540324}\n"
     ]
    }
   ],
   "source": [
    "### BPE - Balance classes\n",
    "classes = np.sort(y_train.unique())\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "\n",
    "class_weights = {int(k): float(v) for k, v in zip(classes, weights)}\n",
    "print(f\"Class weights: {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ce039",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BPE Model Definition\n",
    "def build_transformer_classifier(vocab_size, num_classes, max_len=128, num_heads=4, feed_forward_dim=256, rate=0.1):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim=128)\n",
    "    X = embedding_layer(input_ids)\n",
    "    X = TransformerBlock(embed_dim=128, num_heads=num_heads, feed_forward_dim=feed_forward_dim, rate=rate)(X, training=True, mask=input_mask)\n",
    "\n",
    "    X = tf.keras.layers.GlobalAveragePooling1D()(X)\n",
    "    X = tf.keras.layers.Dropout(rate)(X)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(num_classes, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18044a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Vocab size: 8000\n",
      "Number of classes: 6\n"
     ]
    }
   ],
   "source": [
    "### BPE Tokenizer and Vocab size\n",
    "\n",
    "num_classes = y.nunique()\n",
    "\n",
    "bpe_vocab_size = sp_bpe.get_piece_size()\n",
    "\n",
    "print(f\"BPE Vocab size: {bpe_vocab_size}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce15e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\layer.py:982: UserWarning: Layer 'transformer_block_1' (of type TransformerBlock) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5211/5211\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 181ms/step - accuracy: 0.8729 - loss: 0.2798 - val_accuracy: 0.9050 - val_loss: 0.1954\n",
      "Epoch 2/5\n",
      "\u001b[1m5211/5211\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m877s\u001b[0m 168ms/step - accuracy: 0.9051 - loss: 0.1762 - val_accuracy: 0.9089 - val_loss: 0.1857\n",
      "Epoch 3/5\n",
      "\u001b[1m5211/5211\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m867s\u001b[0m 166ms/step - accuracy: 0.9079 - loss: 0.1681 - val_accuracy: 0.9053 - val_loss: 0.2020\n",
      "Epoch 4/5\n",
      "\u001b[1m5211/5211\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 163ms/step - accuracy: 0.9111 - loss: 0.1606 - val_accuracy: 0.9056 - val_loss: 0.1942\n",
      "Epoch 5/5\n",
      "\u001b[1m5211/5211\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m852s\u001b[0m 164ms/step - accuracy: 0.9143 - loss: 0.1547 - val_accuracy: 0.9047 - val_loss: 0.2033\n"
     ]
    }
   ],
   "source": [
    "### BPE Train ü•≥ü•≥ü•≥\n",
    "\n",
    "bpe_model = build_transformer_classifier(vocab_size=bpe_vocab_size, num_classes=num_classes, max_len=TOKEN_LENGTH)\n",
    "\n",
    "bpe_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    "\n",
    "history = bpe_model.fit(bpe_train_data, validation_data=bpe_val_data, epochs=5, class_weight=class_weights, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0db0ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m652/652\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 53ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9713    0.9228    0.9465     12119\n",
      "           1     0.9839    0.8724    0.9248     14107\n",
      "           2     0.7080    0.9835    0.8233      3456\n",
      "           3     0.9017    0.9204    0.9110      5731\n",
      "           4     0.8605    0.8795    0.8699      4771\n",
      "           5     0.6731    0.9586    0.7909      1497\n",
      "\n",
      "    accuracy                         0.9068     41681\n",
      "   macro avg     0.8498    0.9229    0.8777     41681\n",
      "weighted avg     0.9208    0.9068    0.9097     41681\n",
      "\n",
      "[[11184    88    85   413   308    41]\n",
      " [  144 12307  1250    97    95   214]\n",
      " [   17    14  3399     8     4    14]\n",
      " [   92    52    39  5275   264     9]\n",
      " [   60    28    20    48  4196   419]\n",
      " [   17    19     8     9     9  1435]]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred_probs = bpe_model.predict(bpe_test_data)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
