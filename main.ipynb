{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6728f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tokenization_and_embedding import TokenAndPositionEmbedding\n",
    "from transformer_block import TransformerBlock\n",
    "\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# Dataset https://www.kaggle.com/code/aadyasingh55/model-training-of-tweet-emotion-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc3e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  i feel awful about it too because it s my job ...      0\n",
      "1                              im alone i feel awful      0\n",
      "2  ive probably mentioned this before but i reall...      1\n",
      "3           i was feeling a little low few days back      0\n",
      "4  i beleive that i am much more sensitive to oth...      2\n",
      "0         0\n",
      "1         0\n",
      "2         1\n",
      "3         0\n",
      "4         2\n",
      "         ..\n",
      "416804    1\n",
      "416805    4\n",
      "416806    0\n",
      "416807    1\n",
      "416808    0\n",
      "Name: label, Length: 416809, dtype: int64\n",
      "                                                text  emotion\n",
      "0  i feel awful about it too because it s my job ...  sadness\n",
      "1                              im alone i feel awful  sadness\n",
      "2  ive probably mentioned this before but i reall...      joy\n",
      "3           i was feeling a little low few days back  sadness\n",
      "4  i beleive that i am much more sensitive to oth...     love\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data.parquet')\n",
    "\n",
    "print(df.head())\n",
    "print(df['label'])\n",
    "\n",
    "# Map the labels to emotion names for better readability (Optional)\n",
    "emotion_map = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
    "df['emotion'] = df['label'].map(emotion_map)\n",
    "\n",
    "# Preview the updated dataframe\n",
    "print(df[['text', 'emotion']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc628890",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants\n",
    "EIGHT_THOUSAND = 8000\n",
    "SIXTEEN_THOUSAND = 16000\n",
    "TRAIN_TEXT = 'train_text.txt'\n",
    "LABEL_FILE = 'train_labels.txt'\n",
    "SAMPLE_TEXT = \"This is a sample sentence used for BPE bits. It can be up to 128 characters long.\"\n",
    "TOKEN_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773bc9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416809, 3)\n",
      "Index(['text', 'label', 'emotion'], dtype='object')\n",
      "Data Types: text       object\n",
      "label       int64\n",
      "emotion    object\n",
      "dtype: object\n",
      "Empty values text       0\n",
      "label      0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "Duplicates: 686\n",
      "label\n",
      "1    141067\n",
      "0    121187\n",
      "3     57317\n",
      "4     47712\n",
      "2     34554\n",
      "5     14972\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    0.338\n",
      "0    0.291\n",
      "3    0.138\n",
      "4    0.114\n",
      "2    0.083\n",
      "5    0.036\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#BASIC PRE PROCESSING\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(f\"Data Types: {df.dtypes}\")\n",
    "print(f\"Empty values {df.isna().sum()}\")\n",
    "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df[\"label\"].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba4e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Distribution (After Stratification)\n",
      "Train(%): label\n",
      "0    29.075\n",
      "1    33.844\n",
      "2     8.290\n",
      "3    13.752\n",
      "4    11.447\n",
      "5     3.592\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Val(%): label\n",
      "0    29.076\n",
      "1    33.845\n",
      "2     8.289\n",
      "3    13.752\n",
      "4    11.446\n",
      "5     3.592\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test(%): label\n",
      "0    29.076\n",
      "1    33.845\n",
      "2     8.292\n",
      "3    13.750\n",
      "4    11.446\n",
      "5     3.592\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"text\"].astype(str)\n",
    "y = df[\"label\"].astype(int)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Second split to obtain validation as well as test set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Feature Distribution (After Stratification)\")\n",
    "print(f\"Train(%): {y_train.value_counts(normalize=True).round(5).sort_index() * 100}\\n\")\n",
    "print(f\"Val(%): {y_val.value_counts(normalize=True).round(5).sort_index() * 100}\\n\")\n",
    "print(f\"Test(%): {y_test.value_counts(normalize=True).round(5).sort_index() * 100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67619352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    333447.000000\n",
      "mean         19.200329\n",
      "std          11.046795\n",
      "min           1.000000\n",
      "25%          11.000000\n",
      "50%          17.000000\n",
      "75%          25.000000\n",
      "max         178.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "text_lengths = X_train.str.split().apply(len)\n",
    "print(text_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5226959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 333447 to file\n"
     ]
    }
   ],
   "source": [
    "X_train_clean = X_train.str.strip()\n",
    "X_val_clean = X_val.str.strip()\n",
    "X_test_clean = X_test.str.strip()\n",
    "\n",
    "X_train_clean.to_csv('train_text.txt', index=False, header=False)\n",
    "print(f\"Wrote {len(X_train_clean)} to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc2bee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8000\n",
      "BPE Pieces: ['‚ñÅ', 'T', 'h', 'is', '‚ñÅis', '‚ñÅa', '‚ñÅsam', 'ple', '‚ñÅsentence', '‚ñÅused', '‚ñÅfor', '‚ñÅ', 'BPE', '‚ñÅbits', '.', '‚ñÅ', 'I', 't', '‚ñÅcan', '‚ñÅbe']\n"
     ]
    }
   ],
   "source": [
    "### Byte Pair Encoding (BPE)\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=TRAIN_TEXT,\n",
    "    model_prefix='m_bpe',\n",
    "    vocab_size=EIGHT_THOUSAND,\n",
    "    model_type='bpe'\n",
    ")\n",
    "\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('m_bpe.model')\n",
    "\n",
    "print(f\"Vocab size: {sp_bpe.get_piece_size()}\")\n",
    "print(f\"BPE Pieces: {sp_bpe.encode(SAMPLE_TEXT, out_type=str)[:20]}\")\n",
    "\n",
    "def encode_texts(sp, texts):\n",
    "  \"\"\"\n",
    "  Encode text, create token IDs and attention masks\n",
    "  \"\"\"\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "\n",
    "  for text in texts:\n",
    "    ids = sp.encode(text, out_type=int)\n",
    "\n",
    "    # reduce tokens to 128 (we have some at 178, but minimal)\n",
    "    ids = ids[:TOKEN_LENGTH]\n",
    "    attention_mask = [1] * len(ids)\n",
    "    \n",
    "    # pad to max_len\n",
    "    pad_id = 3\n",
    "    while len(ids) < TOKEN_LENGTH:\n",
    "      ids.append(pad_id)\n",
    "      ## add padding to the attention mask to ensure each token is 128 bits\n",
    "      attention_mask.append(0)\n",
    "\n",
    "    input_ids.append(ids)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "  return np.array(input_ids, dtype=np.int32), np.array(attention_masks, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a0ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE train shape: ((333447, 128), (333447, 128))\n",
      "BPE val shape: ((41681, 128), (41681, 128))\n",
      "BPE test shape: ((41681, 128), (41681, 128))\n"
     ]
    }
   ],
   "source": [
    "### BPE Generate data\n",
    "\n",
    "Xb_train, att_b_mask_train = encode_texts(sp_bpe, X_train_clean.tolist())\n",
    "Xb_val, att_b_mask_val = encode_texts(sp_bpe, X_val_clean.tolist())\n",
    "Xb_test, att_b_mask_test = encode_texts(sp_bpe, X_test_clean.tolist())\n",
    "\n",
    "print(f\"BPE train shape: {Xb_train.shape, att_b_mask_train.shape}\")\n",
    "print(f\"BPE val shape: {Xb_val.shape, att_b_mask_val.shape}\")\n",
    "print(f\"BPE test shape: {Xb_test.shape, att_b_mask_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97b11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BPE TensorFlow input \n",
    "batch_size = 64\n",
    "\n",
    "def make_dataset(X_ids, X_mask, y):\n",
    "  data = tf.data.Dataset.from_tensor_slices(((X_ids, X_mask), y))\n",
    "  data = data.shuffle(10000, reshuffle_each_iteration=True)\n",
    "  data = data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "  return data\n",
    "\n",
    "bpe_train_data = make_dataset(Xb_train, att_b_mask_train, y_train.values)\n",
    "\n",
    "# tf.data.AUTOTUNE allows TensorFlow to automatically determine the optimal number of parallel calls for data loading and preprocessing\n",
    "bpe_val_data = tf.data.Dataset.from_tensor_slices(((Xb_val, att_b_mask_val), y_val.values)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "bpe_test_data = tf.data.Dataset.from_tensor_slices(((Xb_test, att_b_mask_test), y_test.values)).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d9c1a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.5732343809631868, 1: 0.4924503557725537, 2: 2.010436638570343, 3: 1.2119880490251669, 4: 1.455973277443018, 5: 4.639714476540324}\n"
     ]
    }
   ],
   "source": [
    "### BPE - Balance classes\n",
    "classes = np.sort(y_train.unique())\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "\n",
    "class_weights = {int(k): float(v) for k, v in zip(classes, weights)}\n",
    "print(f\"Class weights: {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "618ce039",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BPE Model Definition\n",
    "def build_transformer_classifier(vocab_size, num_classes, max_len=128, num_heads=4, feed_forward_dim=256, rate=0.1):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim=128)\n",
    "    X = embedding_layer(input_ids)\n",
    "    X = TransformerBlock(embed_dim=128, num_heads=num_heads, feed_forward_dim=feed_forward_dim, rate=rate)(X, tensor_mask=input_mask)\n",
    "\n",
    "    X = tf.keras.layers.GlobalAveragePooling1D()(X)\n",
    "    X = tf.keras.layers.Dropout(rate)(X)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(X)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18044a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Vocab size: 8000\n",
      "Number of classes: 6\n"
     ]
    }
   ],
   "source": [
    "### BPE Tokenizer and Vocab size\n",
    "num_classes = y.nunique()\n",
    "bpe_vocab_size = sp_bpe.get_piece_size()\n",
    "\n",
    "print(f\"BPE Vocab size: {bpe_vocab_size}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce15e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5211/5211 [==============================] - 1339s 257ms/step - loss: 0.2803 - accuracy: 0.8731 - val_loss: 0.2234 - val_accuracy: 0.9014\n",
      "Epoch 2/5\n",
      "5211/5211 [==============================] - 1334s 256ms/step - loss: 0.1754 - accuracy: 0.9052 - val_loss: 0.2013 - val_accuracy: 0.9048\n",
      "Epoch 3/5\n",
      "5211/5211 [==============================] - 1345s 258ms/step - loss: 0.1675 - accuracy: 0.9080 - val_loss: 0.2075 - val_accuracy: 0.9055\n",
      "Epoch 4/5\n",
      "5211/5211 [==============================] - 1347s 259ms/step - loss: 0.1602 - accuracy: 0.9111 - val_loss: 0.2031 - val_accuracy: 0.9060\n",
      "Epoch 5/5\n",
      "5211/5211 [==============================] - 1339s 257ms/step - loss: 0.1536 - accuracy: 0.9145 - val_loss: 0.2312 - val_accuracy: 0.9042\n"
     ]
    }
   ],
   "source": [
    "### BPE Train ü•≥ü•≥ü•≥\n",
    "bpe_model = build_transformer_classifier(vocab_size=bpe_vocab_size, num_classes=num_classes, max_len=TOKEN_LENGTH)\n",
    "\n",
    "bpe_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
    "\n",
    "history = bpe_model.fit(bpe_train_data, validation_data=bpe_val_data, epochs=5, class_weight=class_weights, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0db0ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652/652 [==============================] - 91s 139ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9849    0.9055    0.9436     12119\n",
      "           1     0.9789    0.8739    0.9234     14107\n",
      "           2     0.7264    0.9656    0.8291      3456\n",
      "           3     0.8937    0.9274    0.9103      5731\n",
      "           4     0.8390    0.8944    0.8658      4771\n",
      "           5     0.6368    0.9860    0.7738      1497\n",
      "\n",
      "    accuracy                         0.9044     41681\n",
      "   macro avg     0.8433    0.9255    0.8743     41681\n",
      "weighted avg     0.9197    0.9044    0.9077     41681\n",
      "\n",
      "[[10974   149    57   478   396    65]\n",
      " [   91 12328  1180    89   136   283]\n",
      " [   21    30  3337    19    14    35]\n",
      " [   34    73    18  5315   269    22]\n",
      " [   13    11     2    41  4267   437]\n",
      " [    9     3     0     5     4  1476]]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred_probs = bpe_model.predict(bpe_test_data)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f1ad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(bpe_model, to_file='bpe_model.png', show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-venv)",
   "language": "python",
   "name": "dl-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
